{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ca331b2",
   "metadata": {},
   "source": [
    "# Load dataset & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20b9b46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "root_dir = \"datasets/infuluenza/\"\n",
    "data_path = root_dir + \"Influenza2.csv\"\n",
    "exp_dir = \"datasets/infuluenza/vecLSTMx2/\"\n",
    "\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6b34ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alltime</th>\n",
       "      <th>Time</th>\n",
       "      <th>region</th>\n",
       "      <th>regnames</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Altitude</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Week</th>\n",
       "      <th>Flucases</th>\n",
       "      <th>Holiday</th>\n",
       "      <th>Tempave</th>\n",
       "      <th>Tempmin</th>\n",
       "      <th>Tempmax</th>\n",
       "      <th>Rh</th>\n",
       "      <th>Rainfall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12480</th>\n",
       "      <td>12481</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>35.689185</td>\n",
       "      <td>139.691648</td>\n",
       "      <td>34.9</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>818</td>\n",
       "      <td>2</td>\n",
       "      <td>9.528571</td>\n",
       "      <td>6.371429</td>\n",
       "      <td>13.385714</td>\n",
       "      <td>50.571429</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12481</th>\n",
       "      <td>12482</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>35.689185</td>\n",
       "      <td>139.691648</td>\n",
       "      <td>34.9</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1167</td>\n",
       "      <td>3</td>\n",
       "      <td>8.642857</td>\n",
       "      <td>5.457143</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>61.142857</td>\n",
       "      <td>41.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12482</th>\n",
       "      <td>12483</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>35.689185</td>\n",
       "      <td>139.691648</td>\n",
       "      <td>34.9</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2562</td>\n",
       "      <td>2</td>\n",
       "      <td>6.828571</td>\n",
       "      <td>4.171429</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12483</th>\n",
       "      <td>12484</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>35.689185</td>\n",
       "      <td>139.691648</td>\n",
       "      <td>34.9</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3781</td>\n",
       "      <td>2</td>\n",
       "      <td>5.342857</td>\n",
       "      <td>1.542857</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12484</th>\n",
       "      <td>12485</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>35.689185</td>\n",
       "      <td>139.691648</td>\n",
       "      <td>34.9</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3903</td>\n",
       "      <td>2</td>\n",
       "      <td>7.214286</td>\n",
       "      <td>3.171429</td>\n",
       "      <td>11.585714</td>\n",
       "      <td>38.714286</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Alltime  Time  region regnames  Longitude    Latitude  Altitude  Year  \\\n",
       "12480    12481     1      13    Tokyo  35.689185  139.691648      34.9  2000   \n",
       "12481    12482     2      13    Tokyo  35.689185  139.691648      34.9  2000   \n",
       "12482    12483     3      13    Tokyo  35.689185  139.691648      34.9  2000   \n",
       "12483    12484     4      13    Tokyo  35.689185  139.691648      34.9  2000   \n",
       "12484    12485     5      13    Tokyo  35.689185  139.691648      34.9  2000   \n",
       "\n",
       "       Month  Week  Flucases  Holiday   Tempave   Tempmin    Tempmax  \\\n",
       "12480      1     1       818        2  9.528571  6.371429  13.385714   \n",
       "12481      1     2      1167        3  8.642857  5.457143  12.000000   \n",
       "12482      1     3      2562        2  6.828571  4.171429   9.500000   \n",
       "12483      1     4      3781        2  5.342857  1.542857   9.400000   \n",
       "12484      1     5      3903        2  7.214286  3.171429  11.585714   \n",
       "\n",
       "              Rh  Rainfall  \n",
       "12480  50.571429       1.0  \n",
       "12481  61.142857      41.5  \n",
       "12482  56.000000      24.0  \n",
       "12483  42.000000       0.0  \n",
       "12484  38.714286       0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokyo = df[df['regnames'].isin([\"Tokyo\"])]\n",
    "\n",
    "df_tokyo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaef683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt #描画用ライブラリ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07d66e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(780, 260)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_n = int(len(df_tokyo[\"Flucases\"])*0.75)\n",
    "test_n = int(len(df_tokyo[\"Flucases\"])*0.25)\n",
    "train_n, test_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e5f1dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = np.arange(train_n)\n",
    "test_steps = np.arange(train_n,train_n + test_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfa21ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Flucases\",\"Tempave\", \"Rh\", \"Holiday\", \"Rainfall\"]\n",
    "\n",
    "all_data = df_tokyo[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15a2acb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = all_data[columns][0:train_n]\n",
    "test_df = all_data[columns][train_n:train_n+test_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82e63f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flucases</th>\n",
       "      <th>Tempave</th>\n",
       "      <th>Rh</th>\n",
       "      <th>Holiday</th>\n",
       "      <th>Rainfall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12480</th>\n",
       "      <td>818</td>\n",
       "      <td>9.528571</td>\n",
       "      <td>50.571429</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12481</th>\n",
       "      <td>1167</td>\n",
       "      <td>8.642857</td>\n",
       "      <td>61.142857</td>\n",
       "      <td>3</td>\n",
       "      <td>41.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12482</th>\n",
       "      <td>2562</td>\n",
       "      <td>6.828571</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12483</th>\n",
       "      <td>3781</td>\n",
       "      <td>5.342857</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12484</th>\n",
       "      <td>3903</td>\n",
       "      <td>7.214286</td>\n",
       "      <td>38.714286</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Flucases   Tempave         Rh  Holiday  Rainfall\n",
       "12480       818  9.528571  50.571429        2       1.0\n",
       "12481      1167  8.642857  61.142857        3      41.5\n",
       "12482      2562  6.828571  56.000000        2      24.0\n",
       "12483      3781  5.342857  42.000000        2       0.0\n",
       "12484      3903  7.214286  38.714286        2       0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "686fe0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flucases</th>\n",
       "      <th>Tempave</th>\n",
       "      <th>Rh</th>\n",
       "      <th>Holiday</th>\n",
       "      <th>Rainfall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13260</th>\n",
       "      <td>3809</td>\n",
       "      <td>5.357143</td>\n",
       "      <td>47.142857</td>\n",
       "      <td>3</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13261</th>\n",
       "      <td>9892</td>\n",
       "      <td>6.285714</td>\n",
       "      <td>38.285714</td>\n",
       "      <td>2</td>\n",
       "      <td>35.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13262</th>\n",
       "      <td>8198</td>\n",
       "      <td>5.628571</td>\n",
       "      <td>53.857143</td>\n",
       "      <td>3</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13263</th>\n",
       "      <td>9625</td>\n",
       "      <td>6.771429</td>\n",
       "      <td>63.714286</td>\n",
       "      <td>2</td>\n",
       "      <td>31.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13264</th>\n",
       "      <td>7844</td>\n",
       "      <td>4.228571</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Flucases   Tempave         Rh  Holiday  Rainfall\n",
       "13260      3809  5.357143  47.142857        3       6.5\n",
       "13261      9892  6.285714  38.285714        2      35.5\n",
       "13262      8198  5.628571  53.857143        3      19.0\n",
       "13263      9625  6.771429  63.714286        2      31.5\n",
       "13264      7844  4.228571  49.000000        2       9.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16ff1c5",
   "metadata": {},
   "source": [
    "trainとtestのdfを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff4f21be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ts_df = train_df[\"Flucases\"]\n",
    "train_ys_df = train_df[[\"Tempave\", \"Rh\", \"Rainfall\"]]\n",
    "train_xs_df = train_df[\"Holiday\"]\n",
    "\n",
    "test_ts_df = test_df[\"Flucases\"]\n",
    "test_ys_df = test_df[[\"Tempave\", \"Rh\", \"Rainfall\"]]\n",
    "test_xs_df = test_df[\"Holiday\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8396290a",
   "metadata": {},
   "source": [
    "# Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6674accc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Sequential, Linear, Tanh, Sigmoid, LeakyReLU, ReLU\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np #数値計算用ライブラリ\n",
    "import matplotlib.pyplot as plt #描画用ライブラリ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3eb91c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"datasets/infuluenza/\"\n",
    "data_path = root_dir + \"Influenza2.csv\"\n",
    "data_dir = \"datasets/infuluenza/vecLSTMx2/\"\n",
    "exp_dir = \"datasets/infuluenza/vecLSTMx2/\"\n",
    "\n",
    "if not os.path.exists(exp_dir):\n",
    "    os.makedirs(exp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61180435",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 26\n",
    "\n",
    "train_Xs = np.load(data_dir + \"train_Xs_\" + str(seq_length) + \"w.npy\")\n",
    "train_ys = np.load(data_dir + \"train_ys_\" + str(seq_length) + \"w.npy\")\n",
    "val_Xs = np.load(data_dir + \"val_Xs_\" + str(seq_length) + \"w.npy\")\n",
    "val_ys = np.load(data_dir + \"val_ys_\" + str(seq_length) + \"w.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c332768",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_xs_tensor = torch.Tensor(train_Xs)\n",
    "trn_ys_tensor = torch.Tensor(train_ys)\n",
    "\n",
    "val_xs_tensor = torch.Tensor(val_Xs)\n",
    "val_ys_tensor = torch.Tensor(val_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1653719a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((603, 26, 5), (603, 1, 5), (151, 26, 5), (151, 1, 5))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Xs.shape, train_ys.shape, val_Xs.shape, val_ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68c17d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# スケーラーの読み込み\n",
    "std_scaler = joblib.load(exp_dir + 'std_scaler.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0517404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "trn_dataset = TensorDataset(trn_xs_tensor, trn_ys_tensor)\n",
    "trn_loader = DataLoader(dataset=trn_dataset,batch_size=50,shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(val_xs_tensor, val_ys_tensor)\n",
    "val_loader = DataLoader(dataset=val_dataset,batch_size=50,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf0a91f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([50, 26, 5]) torch.Size([50, 1, 5])\n",
      "1 torch.Size([50, 26, 5]) torch.Size([50, 1, 5])\n",
      "2 torch.Size([50, 26, 5]) torch.Size([50, 1, 5])\n",
      "3 torch.Size([50, 26, 5]) torch.Size([50, 1, 5])\n",
      "4 torch.Size([50, 26, 5]) torch.Size([50, 1, 5])\n",
      "5 torch.Size([50, 26, 5]) torch.Size([50, 1, 5])\n",
      "6 torch.Size([50, 26, 5]) torch.Size([50, 1, 5])\n",
      "7 torch.Size([50, 26, 5]) torch.Size([50, 1, 5])\n",
      "8 torch.Size([50, 26, 5]) torch.Size([50, 1, 5])\n",
      "9 torch.Size([50, 26, 5]) torch.Size([50, 1, 5])\n",
      "10 torch.Size([50, 26, 5]) torch.Size([50, 1, 5])\n",
      "11 torch.Size([50, 26, 5]) torch.Size([50, 1, 5])\n",
      "12 torch.Size([3, 26, 5]) torch.Size([3, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "for id, batch_data in enumerate(trn_loader):\n",
    "    \n",
    "    batch_x, batch_y = batch_data\n",
    "    print(id, batch_x.shape, batch_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88cc30cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([50, 26, 5]) torch.Size([50, 1, 5])\n",
      "1 torch.Size([50, 26, 5]) torch.Size([50, 1, 5])\n",
      "2 torch.Size([50, 26, 5]) torch.Size([50, 1, 5])\n",
      "3 torch.Size([1, 26, 5]) torch.Size([1, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "for id, batch_data in enumerate(val_loader):\n",
    "    \n",
    "    batch_x, batch_y = batch_data\n",
    "    print(id, batch_x.shape, batch_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd8414d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78fc005b",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "877584c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cLSTM(nn.Module):\n",
    "    def __init__(self, input_size = 4, hidden_size=32, cond_size = 1, output_size=4, num_layers = 2):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.cond_size = cond_size\n",
    "        self.n_layers = num_layers\n",
    "        \n",
    "        self.relu = ReLU()\n",
    "        self.lrelu = LeakyReLU(negative_slope=0.01)\n",
    "        \n",
    "        # input_sizeは入力する次元数\n",
    "        self.lstm = nn.LSTM(input_size=self.input_size, num_layers=self.n_layers, hidden_size=self.hidden_size, dropout=0.2)\n",
    "        \n",
    "        self.cfc1 = nn.Linear(self.cond_size, int(self.hidden_size / 2))\n",
    "        self.cfc2 = nn.Linear(int(self.hidden_size / 2), int(self.hidden_size / 4))\n",
    "        \n",
    "        self.fc1 = nn.Linear(int(self.hidden_size / 4) + self.hidden_size, self.hidden_size * 4)\n",
    "        self.fc2 = nn.Linear(self.hidden_size * 4, self.output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, cin):\n",
    "        last_outs, hidden = self.lstm(x) #(batch_size, seq_len, hidden)\n",
    "        outs = last_outs[:, -1, :] #(batch_size, hidden)\n",
    "        \n",
    "        ch1 = self.relu(self.cfc1(cin))\n",
    "        ch2 = self.cfc2(ch1)\n",
    "        \n",
    "        h_list = [outs, ch2]\n",
    "        h1 = torch.cat(h_list, dim=1)\n",
    "        h2 = self.relu(self.fc1(h1))\n",
    "        h3 = self.fc2(h2)\n",
    "        \n",
    "        y_list = [self.relu(h3[:,0:1]), h3[:,1:3], self.relu(h3[:,3:4])]\n",
    "        ys = torch.cat(y_list, dim=1)\n",
    "\n",
    "        return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf0410ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "model = cLSTM(input_size = 5, hidden_size = 32, cond_size = 1, output_size = 4, num_layers = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e849415",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b71e5e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_0x = list(trn_loader)[0][0]\n",
    "batch_0c = list(trn_loader)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1dfc1bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50, 26, 5]), torch.Size([50, 1, 5]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_0x.shape, batch_0c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c21904ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_0c = torch.reshape(batch_0c[:,:,-1], (batch_0c.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26082994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_0c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e06976c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(batch_0x, batch_0c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "660b7da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 4])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3470ebf",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d463aa23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cLSTM(\n",
       "  (relu): ReLU()\n",
       "  (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "  (lstm): LSTM(5, 32, dropout=0.2)\n",
       "  (cfc1): Linear(in_features=1, out_features=16, bias=True)\n",
       "  (cfc2): Linear(in_features=16, out_features=8, bias=True)\n",
       "  (fc1): Linear(in_features=40, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7cc6e8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction='none').cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76413d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_model(epoch):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    sum_errors = 0\n",
    "    \n",
    "    for batch_data in val_loader:\n",
    "       \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                inputs, trues = batch_data\n",
    "                inputs, trues = inputs.cuda(), trues.cuda()\n",
    "                input_c = torch.reshape(trues[:,:,-1], (trues.shape[0], 1))\n",
    "\n",
    "            preds = model(inputs, input_c)\n",
    "            true_ys = torch.reshape(trues[:,:,0:4], (trues.shape[0], 4))\n",
    "            loss = criterion(preds, true_ys).mean(dim=0).mean()\n",
    "            \n",
    "            sum_errors = sum_errors + loss.data.item()\n",
    "\n",
    "            del loss\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    return sum_errors / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c9fde42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 Train loss:  1.0553747919889598 Val loss:  0.6042166636325419\n",
      "epoch:  1 Train loss:  0.848025001012362 Val loss:  0.5825243145227432\n",
      "epoch:  2 Train loss:  0.7911351323127747 Val loss:  0.5305329710245132\n",
      "epoch:  3 Train loss:  0.7294332522612351 Val loss:  0.42585499212145805\n",
      "epoch:  4 Train loss:  0.563004039801084 Val loss:  0.3137581255286932\n",
      "epoch:  5 Train loss:  0.42583996859880596 Val loss:  0.27409852668643\n",
      "epoch:  6 Train loss:  0.41591410453502947 Val loss:  0.24980076868087053\n",
      "epoch:  7 Train loss:  0.405341964501601 Val loss:  0.23819652758538723\n",
      "epoch:  8 Train loss:  0.3745344854318179 Val loss:  0.22566192597150803\n",
      "epoch:  9 Train loss:  0.3594092302597486 Val loss:  0.21790224686264992\n",
      "epoch:  10 Train loss:  0.3374626086308406 Val loss:  0.20768778026103973\n",
      "epoch:  11 Train loss:  0.34673003966991717 Val loss:  0.1943715214729309\n",
      "epoch:  12 Train loss:  0.3154485449194908 Val loss:  0.19281422346830368\n",
      "epoch:  13 Train loss:  0.3183797001838684 Val loss:  0.1811718363314867\n",
      "epoch:  14 Train loss:  0.3217028986949187 Val loss:  0.18112600594758987\n",
      "epoch:  15 Train loss:  0.3368546527165633 Val loss:  0.18515187315642834\n",
      "epoch:  16 Train loss:  0.3172079622745514 Val loss:  0.1817663535475731\n",
      "epoch:  17 Train loss:  0.30707931633179003 Val loss:  0.17701518535614014\n",
      "epoch:  18 Train loss:  0.32702925228155577 Val loss:  0.1835266463458538\n",
      "epoch:  19 Train loss:  0.30263171459619814 Val loss:  0.18947395123541355\n",
      "epoch:  20 Train loss:  0.3022579275644742 Val loss:  0.18579454720020294\n",
      "epoch:  21 Train loss:  0.31195993148363554 Val loss:  0.186304472386837\n",
      "epoch:  22 Train loss:  0.31848937158401197 Val loss:  0.1796955205500126\n",
      "epoch:  23 Train loss:  0.31421650373018706 Val loss:  0.1774727012962103\n",
      "epoch:  24 Train loss:  0.29882793243114764 Val loss:  0.17930649034678936\n",
      "epoch:  25 Train loss:  0.31374943485626805 Val loss:  0.17378884367644787\n",
      "epoch:  26 Train loss:  0.3186882837460591 Val loss:  0.18075445666909218\n",
      "epoch:  27 Train loss:  0.39585837721824646 Val loss:  0.17951072752475739\n",
      "epoch:  28 Train loss:  0.30066751402158004 Val loss:  0.18291978258639574\n",
      "epoch:  29 Train loss:  0.2967226809033981 Val loss:  0.17638743575662374\n",
      "epoch:  30 Train loss:  0.34568635431619793 Val loss:  0.17997272033244371\n",
      "epoch:  31 Train loss:  0.31882888766435474 Val loss:  0.17514004930853844\n",
      "epoch:  32 Train loss:  0.3089509285413302 Val loss:  0.17923047207295895\n",
      "epoch:  33 Train loss:  0.3013257109201871 Val loss:  0.18207594752311707\n",
      "epoch:  34 Train loss:  0.2986992202126063 Val loss:  0.17193219251930714\n",
      "epoch:  35 Train loss:  0.2987211346626282 Val loss:  0.17902537807822227\n",
      "epoch:  36 Train loss:  0.29285524728206486 Val loss:  0.17377217393368483\n",
      "epoch:  37 Train loss:  0.3134905260342818 Val loss:  0.17836110666394234\n",
      "epoch:  38 Train loss:  0.3060702016720405 Val loss:  0.18038862198591232\n",
      "epoch:  39 Train loss:  0.29565546260430264 Val loss:  0.17837812565267086\n",
      "epoch:  40 Train loss:  0.3044789330317424 Val loss:  0.1833304539322853\n",
      "epoch:  41 Train loss:  0.31428353373820966 Val loss:  0.18690750002861023\n",
      "epoch:  42 Train loss:  0.36845390498638153 Val loss:  0.17953266762197018\n",
      "epoch:  43 Train loss:  0.2962051721719595 Val loss:  0.18632911145687103\n",
      "epoch:  44 Train loss:  0.3088088115820518 Val loss:  0.17634601891040802\n",
      "epoch:  45 Train loss:  0.3007080623736748 Val loss:  0.1821560636162758\n",
      "epoch:  46 Train loss:  0.292161233150042 Val loss:  0.18112320825457573\n",
      "epoch:  47 Train loss:  0.2882707706437661 Val loss:  0.18168218806385994\n",
      "epoch:  48 Train loss:  0.2890826200063412 Val loss:  0.18193377181887627\n",
      "epoch:  49 Train loss:  0.2905403639261539 Val loss:  0.1840159986168146\n",
      "epoch:  50 Train loss:  0.30573356266205126 Val loss:  0.1827017441391945\n",
      "epoch:  51 Train loss:  0.29266275465488434 Val loss:  0.18071360513567924\n",
      "epoch:  52 Train loss:  0.3018514754680487 Val loss:  0.17519849259406328\n",
      "epoch:  53 Train loss:  0.29780208376737743 Val loss:  0.18030143342912197\n",
      "epoch:  54 Train loss:  0.3060656006519611 Val loss:  0.17719946429133415\n",
      "epoch:  55 Train loss:  0.3014223655829063 Val loss:  0.17425111681222916\n",
      "epoch:  56 Train loss:  0.2933874141711455 Val loss:  0.1769854985177517\n",
      "epoch:  57 Train loss:  0.2972178344543164 Val loss:  0.17557090520858765\n",
      "epoch:  58 Train loss:  0.29801756602067214 Val loss:  0.17535677924752235\n",
      "epoch:  59 Train loss:  0.3066381892332664 Val loss:  0.1817691046744585\n",
      "epoch:  60 Train loss:  0.3368527820477119 Val loss:  0.1852112077176571\n",
      "epoch:  61 Train loss:  0.2921310577255029 Val loss:  0.1825033687055111\n",
      "epoch:  62 Train loss:  0.33151052548335147 Val loss:  0.17544234171509743\n",
      "epoch:  63 Train loss:  0.3172003122476431 Val loss:  0.18802343122661114\n",
      "epoch:  64 Train loss:  0.2877495294580093 Val loss:  0.18757961317896843\n",
      "epoch:  65 Train loss:  0.33763452562002033 Val loss:  0.17764585092663765\n",
      "epoch:  66 Train loss:  0.3121991547254416 Val loss:  0.18836265802383423\n",
      "epoch:  67 Train loss:  0.2893669350216022 Val loss:  0.17693783529102802\n",
      "epoch:  68 Train loss:  0.2967411061892143 Val loss:  0.18343099765479565\n",
      "epoch:  69 Train loss:  0.31068793741556316 Val loss:  0.17924932949244976\n",
      "epoch:  70 Train loss:  0.29931181096113646 Val loss:  0.18448811769485474\n",
      "epoch:  71 Train loss:  0.30654492057286775 Val loss:  0.185067318379879\n",
      "epoch:  72 Train loss:  0.29165976781111497 Val loss:  0.18257888592779636\n",
      "epoch:  73 Train loss:  0.2934592068195343 Val loss:  0.1809215098619461\n",
      "epoch:  74 Train loss:  0.28755084482523113 Val loss:  0.1918766051530838\n",
      "epoch:  75 Train loss:  0.2881796136498451 Val loss:  0.1829359233379364\n",
      "epoch:  76 Train loss:  0.28810349393349427 Val loss:  0.17911218106746674\n",
      "epoch:  77 Train loss:  0.2989522172854497 Val loss:  0.18282629549503326\n",
      "epoch:  78 Train loss:  0.29733190398949844 Val loss:  0.1870262809097767\n",
      "epoch:  79 Train loss:  0.30006386454288775 Val loss:  0.18656249903142452\n",
      "epoch:  80 Train loss:  0.292763236623544 Val loss:  0.1871671937406063\n",
      "epoch:  81 Train loss:  0.306220150910891 Val loss:  0.190886989235878\n",
      "epoch:  82 Train loss:  0.2977442466295682 Val loss:  0.1874377354979515\n",
      "epoch:  83 Train loss:  0.31429546727583957 Val loss:  0.19110913015902042\n",
      "epoch:  84 Train loss:  0.296361499107801 Val loss:  0.19069183059036732\n",
      "epoch:  85 Train loss:  0.29266086449989903 Val loss:  0.1840126346796751\n",
      "epoch:  86 Train loss:  0.3038901858604871 Val loss:  0.18279743753373623\n",
      "epoch:  87 Train loss:  0.3032195121049881 Val loss:  0.18990156799554825\n",
      "epoch:  88 Train loss:  0.2931028948380397 Val loss:  0.18275384604930878\n",
      "epoch:  89 Train loss:  0.3578721112929858 Val loss:  0.18588636070489883\n",
      "epoch:  90 Train loss:  0.30654408496159774 Val loss:  0.1915991697460413\n",
      "epoch:  91 Train loss:  0.2885378702328755 Val loss:  0.182745561003685\n",
      "epoch:  92 Train loss:  0.3472676219848486 Val loss:  0.19451401755213737\n",
      "epoch:  93 Train loss:  0.29750705109192777 Val loss:  0.19800816848874092\n",
      "epoch:  94 Train loss:  0.28660275557866466 Val loss:  0.19500448741018772\n",
      "epoch:  95 Train loss:  0.2910540115374785 Val loss:  0.18470096588134766\n",
      "epoch:  96 Train loss:  0.2966453604973279 Val loss:  0.18514011427760124\n",
      "epoch:  97 Train loss:  0.2904365532673322 Val loss:  0.19872937723994255\n",
      "epoch:  98 Train loss:  0.31004612606305343 Val loss:  0.18432905338704586\n",
      "epoch:  99 Train loss:  0.31624773603219253 Val loss:  0.20479924604296684\n",
      "epoch:  100 Train loss:  0.283746571208422 Val loss:  0.19038763642311096\n",
      "epoch:  101 Train loss:  0.2960072480715238 Val loss:  0.1836344115436077\n",
      "epoch:  102 Train loss:  0.28154257484353507 Val loss:  0.18222692050039768\n",
      "epoch:  103 Train loss:  0.29106989617531115 Val loss:  0.19039151072502136\n",
      "epoch:  104 Train loss:  0.28558897456297505 Val loss:  0.1819391492754221\n",
      "epoch:  105 Train loss:  0.28171922724980575 Val loss:  0.18782081827521324\n",
      "epoch:  106 Train loss:  0.2858300186120547 Val loss:  0.18473754823207855\n",
      "epoch:  107 Train loss:  0.2859755765933257 Val loss:  0.18218303099274635\n",
      "epoch:  108 Train loss:  0.2797228309970636 Val loss:  0.18888910114765167\n",
      "epoch:  109 Train loss:  0.2825352085324434 Val loss:  0.18718873336911201\n",
      "epoch:  110 Train loss:  0.28386458181417906 Val loss:  0.18139936961233616\n",
      "epoch:  111 Train loss:  0.28784505220559925 Val loss:  0.18222531490027905\n",
      "epoch:  112 Train loss:  0.32104839155307185 Val loss:  0.18256819434463978\n",
      "epoch:  113 Train loss:  0.3092767768181287 Val loss:  0.18768446519970894\n",
      "epoch:  114 Train loss:  0.29678406623693615 Val loss:  0.20191418752074242\n",
      "epoch:  115 Train loss:  0.28405878234368104 Val loss:  0.18458221293985844\n",
      "epoch:  116 Train loss:  0.2994347524184447 Val loss:  0.18686982616782188\n",
      "epoch:  117 Train loss:  0.29326454951212955 Val loss:  0.19485795125365257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  118 Train loss:  0.28373761761647004 Val loss:  0.18673870339989662\n",
      "epoch:  119 Train loss:  0.28398190438747406 Val loss:  0.18083380162715912\n",
      "epoch:  120 Train loss:  0.28044848144054413 Val loss:  0.18362712860107422\n",
      "epoch:  121 Train loss:  0.2866024902233711 Val loss:  0.18555640429258347\n",
      "epoch:  122 Train loss:  0.28552883061078876 Val loss:  0.1892264559864998\n",
      "epoch:  123 Train loss:  0.29123405195199525 Val loss:  0.1859239935874939\n",
      "epoch:  124 Train loss:  0.2869053528859065 Val loss:  0.18137619644403458\n",
      "epoch:  125 Train loss:  0.2982917966750952 Val loss:  0.1865882147103548\n",
      "epoch:  126 Train loss:  0.2916490320975964 Val loss:  0.18297762051224709\n",
      "epoch:  127 Train loss:  0.28735747933387756 Val loss:  0.18743409402668476\n",
      "epoch:  128 Train loss:  0.28708727543170637 Val loss:  0.18682657927274704\n",
      "epoch:  129 Train loss:  0.27937823419387525 Val loss:  0.18518532812595367\n",
      "epoch:  130 Train loss:  0.3422665217748055 Val loss:  0.186319412663579\n",
      "epoch:  131 Train loss:  0.2918445158463258 Val loss:  0.1899751890450716\n",
      "epoch:  132 Train loss:  0.2887744972339043 Val loss:  0.1958459224551916\n",
      "epoch:  133 Train loss:  0.29671175663287824 Val loss:  0.1915588155388832\n",
      "epoch:  134 Train loss:  0.28073628361408526 Val loss:  0.19210522808134556\n",
      "epoch:  135 Train loss:  0.2961497868482883 Val loss:  0.19051042571663857\n",
      "epoch:  136 Train loss:  0.28713622803871447 Val loss:  0.1908610723912716\n",
      "epoch:  137 Train loss:  0.2938604847742961 Val loss:  0.18616357631981373\n",
      "epoch:  138 Train loss:  0.29181915865494656 Val loss:  0.19156024232506752\n",
      "epoch:  139 Train loss:  0.2906614255446654 Val loss:  0.18564065173268318\n",
      "epoch:  140 Train loss:  0.2802247754656352 Val loss:  0.18687138333916664\n",
      "epoch:  141 Train loss:  0.2918456345796585 Val loss:  0.19161798246204853\n",
      "epoch:  142 Train loss:  0.29090578166338116 Val loss:  0.18767620623111725\n",
      "epoch:  143 Train loss:  0.28720309184147763 Val loss:  0.19444705732166767\n",
      "epoch:  144 Train loss:  0.28223852583995235 Val loss:  0.1851555947214365\n",
      "epoch:  145 Train loss:  0.29325140095674074 Val loss:  0.19603504240512848\n",
      "epoch:  146 Train loss:  0.2881954713509633 Val loss:  0.1943029835820198\n",
      "epoch:  147 Train loss:  0.365675429885204 Val loss:  0.19476279243826866\n",
      "epoch:  148 Train loss:  0.2806146672138801 Val loss:  0.20130180194973946\n",
      "epoch:  149 Train loss:  0.2942024377676157 Val loss:  0.18503624014556408\n",
      "epoch:  150 Train loss:  0.28168678054442775 Val loss:  0.19543639943003654\n",
      "epoch:  151 Train loss:  0.28515666952499974 Val loss:  0.18502666987478733\n",
      "epoch:  152 Train loss:  0.2989887102292134 Val loss:  0.18511097133159637\n",
      "epoch:  153 Train loss:  0.2832778680783052 Val loss:  0.18621256574988365\n",
      "epoch:  154 Train loss:  0.28540044793715846 Val loss:  0.19151047058403492\n",
      "epoch:  155 Train loss:  0.28152444614813876 Val loss:  0.19115140475332737\n",
      "epoch:  156 Train loss:  0.2918379134856738 Val loss:  0.18261704593896866\n",
      "epoch:  157 Train loss:  0.27927401203375596 Val loss:  0.19132016599178314\n",
      "epoch:  158 Train loss:  0.28266586248691267 Val loss:  0.1878989152610302\n",
      "epoch:  159 Train loss:  0.27816062477918774 Val loss:  0.18780691176652908\n",
      "epoch:  160 Train loss:  0.27899060341028065 Val loss:  0.18620236590504646\n",
      "epoch:  161 Train loss:  0.2920455588744237 Val loss:  0.1919847708195448\n",
      "epoch:  162 Train loss:  0.28362600390727705 Val loss:  0.18871325999498367\n",
      "epoch:  163 Train loss:  0.28329829298532927 Val loss:  0.18535235337913036\n",
      "epoch:  164 Train loss:  0.29940323760876286 Val loss:  0.19254190474748611\n",
      "epoch:  165 Train loss:  0.31725748914938706 Val loss:  0.1922989059239626\n",
      "epoch:  166 Train loss:  0.27948397397994995 Val loss:  0.19518963620066643\n",
      "epoch:  167 Train loss:  0.2804845737723204 Val loss:  0.19563360512256622\n",
      "epoch:  168 Train loss:  0.40494337104834044 Val loss:  0.2003762386739254\n",
      "epoch:  169 Train loss:  0.2803045574289102 Val loss:  0.2037152424454689\n",
      "epoch:  170 Train loss:  0.2842530527940163 Val loss:  0.19383908063173294\n",
      "epoch:  171 Train loss:  0.2790125665756372 Val loss:  0.1901983544230461\n",
      "epoch:  172 Train loss:  0.28059348120139194 Val loss:  0.18853389658033848\n",
      "epoch:  173 Train loss:  0.27375812370043534 Val loss:  0.19657103158533573\n",
      "epoch:  174 Train loss:  0.2813192777908765 Val loss:  0.18730858899652958\n",
      "epoch:  175 Train loss:  0.2910767300770833 Val loss:  0.185664314776659\n",
      "epoch:  176 Train loss:  0.2885334606354053 Val loss:  0.19154205173254013\n",
      "epoch:  177 Train loss:  0.3142593835408871 Val loss:  0.18443635292351246\n",
      "epoch:  178 Train loss:  0.28276754858402103 Val loss:  0.19318120926618576\n",
      "epoch:  179 Train loss:  0.2809121631658994 Val loss:  0.18663982674479485\n",
      "epoch:  180 Train loss:  0.2732054992364003 Val loss:  0.18968289904296398\n",
      "epoch:  181 Train loss:  0.2793179303407669 Val loss:  0.18209875375032425\n",
      "epoch:  182 Train loss:  0.2883736766301669 Val loss:  0.18454143591225147\n",
      "epoch:  183 Train loss:  0.2847272214981226 Val loss:  0.17859137058258057\n",
      "epoch:  184 Train loss:  0.28151945769786835 Val loss:  0.1821441799402237\n",
      "epoch:  185 Train loss:  0.2730348293597882 Val loss:  0.17869824916124344\n",
      "epoch:  186 Train loss:  0.27420420142320484 Val loss:  0.1820278111845255\n",
      "epoch:  187 Train loss:  0.29172021035964674 Val loss:  0.18385292030870914\n",
      "epoch:  188 Train loss:  0.2802421645476268 Val loss:  0.18218193762004375\n",
      "epoch:  189 Train loss:  0.29014014395383686 Val loss:  0.18419067934155464\n",
      "epoch:  190 Train loss:  0.2834168661099214 Val loss:  0.19067299365997314\n",
      "epoch:  191 Train loss:  0.28130571429546064 Val loss:  0.18108451552689075\n",
      "epoch:  192 Train loss:  0.28698184283880085 Val loss:  0.18440860696136951\n",
      "epoch:  193 Train loss:  0.2898272310311978 Val loss:  0.1865565087646246\n",
      "epoch:  194 Train loss:  0.2760605296263328 Val loss:  0.18915230967104435\n",
      "epoch:  195 Train loss:  0.27739088008036983 Val loss:  0.18612135387957096\n",
      "epoch:  196 Train loss:  0.26518605133661854 Val loss:  0.18194644711911678\n",
      "epoch:  197 Train loss:  0.31269232355631316 Val loss:  0.19536657631397247\n",
      "epoch:  198 Train loss:  0.2924891102772493 Val loss:  0.18798932060599327\n",
      "epoch:  199 Train loss:  0.285791366146161 Val loss:  0.18720952607691288\n",
      "epoch:  200 Train loss:  0.2798483394659482 Val loss:  0.1899577584117651\n",
      "epoch:  201 Train loss:  0.3083129983681899 Val loss:  0.18812021985650063\n",
      "epoch:  202 Train loss:  0.27807226318579453 Val loss:  0.19487009942531586\n",
      "epoch:  203 Train loss:  0.31463433916752154 Val loss:  0.20099517703056335\n",
      "epoch:  204 Train loss:  0.46368984763438886 Val loss:  0.20107611641287804\n",
      "epoch:  205 Train loss:  0.28169824182987213 Val loss:  0.21063602715730667\n",
      "epoch:  206 Train loss:  0.28373314096377444 Val loss:  0.19330056197941303\n",
      "epoch:  207 Train loss:  0.27175091665524703 Val loss:  0.19459757022559643\n",
      "epoch:  208 Train loss:  0.344982127730663 Val loss:  0.1941665317863226\n",
      "epoch:  209 Train loss:  0.27125726353663665 Val loss:  0.19657072983682156\n",
      "epoch:  210 Train loss:  0.2725208986264009 Val loss:  0.18754440546035767\n",
      "epoch:  211 Train loss:  0.2788765522149893 Val loss:  0.18999339826405048\n",
      "epoch:  212 Train loss:  0.26880265313845414 Val loss:  0.1925979219377041\n",
      "epoch:  213 Train loss:  0.33584415110257954 Val loss:  0.19196303933858871\n",
      "epoch:  214 Train loss:  0.2960666509775015 Val loss:  0.2056741639971733\n",
      "epoch:  215 Train loss:  0.2813404809970122 Val loss:  0.19400224834680557\n",
      "epoch:  216 Train loss:  0.26721135011086095 Val loss:  0.19653695076704025\n",
      "epoch:  217 Train loss:  0.27123612050826734 Val loss:  0.1961379125714302\n",
      "epoch:  218 Train loss:  0.2804727749182628 Val loss:  0.19504991173744202\n",
      "epoch:  219 Train loss:  0.2697886274411128 Val loss:  0.20199862495064735\n",
      "epoch:  220 Train loss:  0.2831540336975685 Val loss:  0.19271090999245644\n",
      "epoch:  221 Train loss:  0.27206370807611024 Val loss:  0.1988161038607359\n",
      "epoch:  222 Train loss:  0.27027545468165326 Val loss:  0.1917348261922598\n",
      "epoch:  223 Train loss:  0.2662903769658162 Val loss:  0.19471975415945053\n",
      "epoch:  224 Train loss:  0.29110657710295457 Val loss:  0.18727090023458004\n",
      "epoch:  225 Train loss:  0.31458026056106275 Val loss:  0.19819058664143085\n",
      "epoch:  226 Train loss:  0.2790182989377242 Val loss:  0.2020362727344036\n",
      "epoch:  227 Train loss:  0.2685769016926105 Val loss:  0.19475581869482994\n",
      "epoch:  228 Train loss:  0.28626523797328657 Val loss:  0.20250185951590538\n",
      "epoch:  229 Train loss:  0.27426523772569805 Val loss:  0.18873491324484348\n",
      "epoch:  230 Train loss:  0.26854926691605496 Val loss:  0.1883732471615076\n",
      "epoch:  231 Train loss:  0.2774614565647565 Val loss:  0.1904445979744196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  232 Train loss:  0.2671362883769549 Val loss:  0.19304146245121956\n",
      "epoch:  233 Train loss:  0.2844706487197142 Val loss:  0.19566510990262032\n",
      "epoch:  234 Train loss:  0.2674335034993979 Val loss:  0.190914960578084\n",
      "epoch:  235 Train loss:  0.269689788038914 Val loss:  0.19035111367702484\n",
      "epoch:  236 Train loss:  0.2734467410124265 Val loss:  0.19354510493576527\n",
      "epoch:  237 Train loss:  0.27536340860220104 Val loss:  0.18516428396105766\n",
      "epoch:  238 Train loss:  0.27599552044501674 Val loss:  0.19792443700134754\n",
      "epoch:  239 Train loss:  0.26975986476127917 Val loss:  0.19649716280400753\n",
      "epoch:  240 Train loss:  0.29357447761755723 Val loss:  0.19891567528247833\n",
      "epoch:  241 Train loss:  0.2746445479301306 Val loss:  0.19595937803387642\n",
      "epoch:  242 Train loss:  0.290005902831371 Val loss:  0.18866749107837677\n",
      "epoch:  243 Train loss:  0.2760023039120894 Val loss:  0.19734272174537182\n",
      "epoch:  244 Train loss:  0.27097682654857635 Val loss:  0.19424282386898994\n",
      "epoch:  245 Train loss:  0.2888680226527728 Val loss:  0.19740990363061428\n",
      "epoch:  246 Train loss:  0.2762125948300728 Val loss:  0.20052601769566536\n",
      "epoch:  247 Train loss:  0.26910965832380146 Val loss:  0.1968579087406397\n",
      "epoch:  248 Train loss:  0.26795311959890217 Val loss:  0.19145856983959675\n",
      "epoch:  249 Train loss:  0.2704229469482715 Val loss:  0.18797081150114536\n",
      "epoch:  250 Train loss:  0.27002634222690874 Val loss:  0.19496586918830872\n",
      "epoch:  251 Train loss:  0.264268747315957 Val loss:  0.1918662879616022\n",
      "epoch:  252 Train loss:  0.2815562543960718 Val loss:  0.19537585228681564\n",
      "epoch:  253 Train loss:  0.2623586064347854 Val loss:  0.19355166517198086\n",
      "epoch:  254 Train loss:  0.36210730557258314 Val loss:  0.19336944073438644\n",
      "epoch:  255 Train loss:  0.26854808513934797 Val loss:  0.1964691486209631\n",
      "epoch:  256 Train loss:  0.2714772086877089 Val loss:  0.18363434821367264\n",
      "epoch:  257 Train loss:  0.2573872741598349 Val loss:  0.19069729559123516\n",
      "epoch:  258 Train loss:  0.2692328416384183 Val loss:  0.1861385740339756\n",
      "epoch:  259 Train loss:  0.26170849112363964 Val loss:  0.18414798378944397\n",
      "epoch:  260 Train loss:  0.264510745039353 Val loss:  0.18959953635931015\n",
      "epoch:  261 Train loss:  0.2807893741589326 Val loss:  0.18856821581721306\n",
      "epoch:  262 Train loss:  0.27284763753414154 Val loss:  0.18634664453566074\n",
      "epoch:  263 Train loss:  0.2773901281448511 Val loss:  0.1889031697064638\n",
      "epoch:  264 Train loss:  0.2737228950628868 Val loss:  0.1916064117103815\n",
      "epoch:  265 Train loss:  0.26081127845324004 Val loss:  0.18599734641611576\n",
      "epoch:  266 Train loss:  0.25869053716842944 Val loss:  0.18320000544190407\n",
      "epoch:  267 Train loss:  0.26011989151056 Val loss:  0.18467591144144535\n",
      "epoch:  268 Train loss:  0.26973297733526963 Val loss:  0.18599473871290684\n",
      "epoch:  269 Train loss:  0.27460369582359606 Val loss:  0.19159160368144512\n",
      "epoch:  270 Train loss:  0.2639179802857913 Val loss:  0.18506755121052265\n",
      "epoch:  271 Train loss:  0.26339128613471985 Val loss:  0.18960577435791492\n",
      "epoch:  272 Train loss:  0.26649089272205645 Val loss:  0.18702164851129055\n",
      "epoch:  273 Train loss:  0.27210084864726436 Val loss:  0.18304852209985256\n",
      "epoch:  274 Train loss:  0.2548706772235724 Val loss:  0.19181047193706036\n",
      "epoch:  275 Train loss:  0.26835119609649366 Val loss:  0.18266407772898674\n",
      "epoch:  276 Train loss:  0.2560981001991492 Val loss:  0.18178855255246162\n",
      "epoch:  277 Train loss:  0.3413131535053253 Val loss:  0.18598979339003563\n",
      "epoch:  278 Train loss:  0.2675496144936635 Val loss:  0.18951741605997086\n",
      "epoch:  279 Train loss:  0.2727873898469485 Val loss:  0.18236436694860458\n",
      "epoch:  280 Train loss:  0.2615621995467406 Val loss:  0.18575574830174446\n",
      "epoch:  281 Train loss:  0.2518791751219676 Val loss:  0.18161900714039803\n",
      "epoch:  282 Train loss:  0.25955676287412643 Val loss:  0.18368096090853214\n",
      "epoch:  283 Train loss:  0.265330236691695 Val loss:  0.1817170139402151\n",
      "epoch:  284 Train loss:  0.2872254791168066 Val loss:  0.18661363050341606\n",
      "epoch:  285 Train loss:  0.258697598026349 Val loss:  0.18229384906589985\n",
      "epoch:  286 Train loss:  0.30151732151324934 Val loss:  0.18807057663798332\n",
      "epoch:  287 Train loss:  0.26458504796028137 Val loss:  0.18841638043522835\n",
      "epoch:  288 Train loss:  0.2615387084392401 Val loss:  0.1822851486504078\n",
      "epoch:  289 Train loss:  0.2659610074300032 Val loss:  0.18929371237754822\n",
      "epoch:  290 Train loss:  0.2631768641563562 Val loss:  0.1823633499443531\n",
      "epoch:  291 Train loss:  0.26340678104987514 Val loss:  0.19164562597870827\n",
      "epoch:  292 Train loss:  0.26391401199194103 Val loss:  0.18155892565846443\n",
      "epoch:  293 Train loss:  0.3528583405109552 Val loss:  0.19221887923777103\n",
      "epoch:  294 Train loss:  0.2800774837915714 Val loss:  0.1956440769135952\n",
      "epoch:  295 Train loss:  0.2735203676498853 Val loss:  0.18267544079571962\n",
      "epoch:  296 Train loss:  0.25527447347457594 Val loss:  0.19186735898256302\n",
      "epoch:  297 Train loss:  0.2558878855063365 Val loss:  0.18587373569607735\n",
      "epoch:  298 Train loss:  0.2614093617751048 Val loss:  0.19149848073720932\n",
      "epoch:  299 Train loss:  0.2579754040791438 Val loss:  0.18834633007645607\n",
      "epoch:  300 Train loss:  0.2615286134756528 Val loss:  0.19004590809345245\n",
      "epoch:  301 Train loss:  0.25954505571952236 Val loss:  0.1870390698313713\n",
      "epoch:  302 Train loss:  0.28611027621305907 Val loss:  0.1861005648970604\n",
      "epoch:  303 Train loss:  0.2562518475147394 Val loss:  0.1848165038973093\n",
      "epoch:  304 Train loss:  0.2514265953348233 Val loss:  0.18904045596718788\n",
      "epoch:  305 Train loss:  0.2612094718676347 Val loss:  0.18655076250433922\n",
      "epoch:  306 Train loss:  0.2573126909824518 Val loss:  0.19090395607054234\n",
      "epoch:  307 Train loss:  0.2837709440634801 Val loss:  0.19176450930535793\n",
      "epoch:  308 Train loss:  0.2588863762525412 Val loss:  0.1898565087467432\n",
      "epoch:  309 Train loss:  0.25293036091786164 Val loss:  0.18461061269044876\n",
      "epoch:  310 Train loss:  0.250085740421827 Val loss:  0.18554595671594143\n",
      "epoch:  311 Train loss:  0.2976365891786722 Val loss:  0.18367904424667358\n",
      "epoch:  312 Train loss:  0.25409034467660463 Val loss:  0.19087320379912853\n",
      "epoch:  313 Train loss:  0.262270891895661 Val loss:  0.18794628046453\n",
      "epoch:  314 Train loss:  0.2654533042357518 Val loss:  0.18679997324943542\n",
      "epoch:  315 Train loss:  0.2504919982300355 Val loss:  0.17935184203088284\n",
      "epoch:  316 Train loss:  0.25170153035567355 Val loss:  0.18574929237365723\n",
      "epoch:  317 Train loss:  0.2572804127748196 Val loss:  0.1832553744316101\n",
      "epoch:  318 Train loss:  0.265567809343338 Val loss:  0.18208066374063492\n",
      "epoch:  319 Train loss:  0.27614654142123 Val loss:  0.1863058116286993\n",
      "epoch:  320 Train loss:  0.2538210686582785 Val loss:  0.1880651917308569\n",
      "epoch:  321 Train loss:  0.2697884085086676 Val loss:  0.19339523650705814\n",
      "epoch:  322 Train loss:  0.2545884710091811 Val loss:  0.19575900584459305\n",
      "epoch:  323 Train loss:  0.2731736841110083 Val loss:  0.19800953939557076\n",
      "epoch:  324 Train loss:  0.3767894644003648 Val loss:  0.19964872300624847\n",
      "epoch:  325 Train loss:  0.2980961364049178 Val loss:  0.21013844572007656\n",
      "epoch:  326 Train loss:  0.262487581716134 Val loss:  0.19029294699430466\n",
      "epoch:  327 Train loss:  0.2604448359746199 Val loss:  0.1898204367607832\n",
      "epoch:  328 Train loss:  0.25293749341597926 Val loss:  0.17907156608998775\n",
      "epoch:  329 Train loss:  0.2623461290047719 Val loss:  0.18570302613079548\n",
      "epoch:  330 Train loss:  0.2500274914961595 Val loss:  0.18589979223906994\n",
      "epoch:  331 Train loss:  0.26139224148713625 Val loss:  0.18930071219801903\n",
      "epoch:  332 Train loss:  0.2672037963683789 Val loss:  0.1839748304337263\n",
      "epoch:  333 Train loss:  0.2533625341378726 Val loss:  0.18751460686326027\n",
      "epoch:  334 Train loss:  0.24902804883626792 Val loss:  0.18233212269842625\n",
      "epoch:  335 Train loss:  0.26877973744502437 Val loss:  0.18334629200398922\n",
      "epoch:  336 Train loss:  0.2552576248462384 Val loss:  0.18313552252948284\n",
      "epoch:  337 Train loss:  0.2544875924403851 Val loss:  0.1887301243841648\n",
      "epoch:  338 Train loss:  0.2498550105553407 Val loss:  0.18867508694529533\n",
      "epoch:  339 Train loss:  0.2481607164327915 Val loss:  0.1812342330813408\n",
      "epoch:  340 Train loss:  0.2721634931289233 Val loss:  0.18592464178800583\n",
      "epoch:  341 Train loss:  0.2505659357859538 Val loss:  0.17908167280256748\n",
      "epoch:  342 Train loss:  0.24922973834551299 Val loss:  0.1845208089798689\n",
      "epoch:  343 Train loss:  0.2570826548796434 Val loss:  0.18361501581966877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  344 Train loss:  0.2665386429199806 Val loss:  0.1867516040802002\n",
      "epoch:  345 Train loss:  0.2810693784401967 Val loss:  0.1793329445645213\n",
      "epoch:  346 Train loss:  0.24723674528873885 Val loss:  0.19176414050161839\n",
      "epoch:  347 Train loss:  0.281823340516824 Val loss:  0.18136594723910093\n",
      "epoch:  348 Train loss:  0.25832490622997284 Val loss:  0.1887415163218975\n",
      "epoch:  349 Train loss:  0.24874238211375016 Val loss:  0.18289904855191708\n",
      "epoch:  350 Train loss:  0.25631226140719193 Val loss:  0.1849234625697136\n",
      "epoch:  351 Train loss:  0.25106652768758625 Val loss:  0.19169378280639648\n",
      "epoch:  352 Train loss:  0.24371300924282807 Val loss:  0.18608647771179676\n",
      "epoch:  353 Train loss:  0.24591640211068666 Val loss:  0.18614772334694862\n",
      "epoch:  354 Train loss:  0.25557725246136004 Val loss:  0.18518677912652493\n",
      "epoch:  355 Train loss:  0.257849857211113 Val loss:  0.18076466768980026\n",
      "epoch:  356 Train loss:  0.2580717962521773 Val loss:  0.18208057805895805\n",
      "epoch:  357 Train loss:  0.2520901961968495 Val loss:  0.18453282862901688\n",
      "epoch:  358 Train loss:  0.25826213910029483 Val loss:  0.18380834348499775\n",
      "epoch:  359 Train loss:  0.25091176766615647 Val loss:  0.1823607850819826\n",
      "epoch:  360 Train loss:  0.248524832037779 Val loss:  0.18920844234526157\n",
      "epoch:  361 Train loss:  0.245928151676288 Val loss:  0.18387096375226974\n",
      "epoch:  362 Train loss:  0.3336119388158505 Val loss:  0.19411922059953213\n",
      "epoch:  363 Train loss:  0.2544319239946512 Val loss:  0.19871773943305016\n",
      "epoch:  364 Train loss:  0.25545557416402376 Val loss:  0.18534591421484947\n",
      "epoch:  365 Train loss:  0.24987057195259973 Val loss:  0.18689629435539246\n",
      "epoch:  366 Train loss:  0.27797731298666734 Val loss:  0.18593208864331245\n",
      "epoch:  367 Train loss:  0.24614672362804413 Val loss:  0.19001991674304008\n",
      "epoch:  368 Train loss:  0.2641586741575828 Val loss:  0.19288657419383526\n",
      "epoch:  369 Train loss:  0.25068702835303086 Val loss:  0.19748380221426487\n",
      "epoch:  370 Train loss:  0.2508968527500446 Val loss:  0.19466985948383808\n",
      "epoch:  371 Train loss:  0.24558811921339768 Val loss:  0.20110071636736393\n",
      "epoch:  372 Train loss:  0.25563552517157334 Val loss:  0.1899475622922182\n",
      "epoch:  373 Train loss:  0.2459368258714676 Val loss:  0.1929642390459776\n",
      "epoch:  374 Train loss:  0.2502316121871655 Val loss:  0.18674903362989426\n",
      "epoch:  375 Train loss:  0.24904998678427476 Val loss:  0.1869005337357521\n",
      "epoch:  376 Train loss:  0.2484746793141732 Val loss:  0.1854990106076002\n",
      "epoch:  377 Train loss:  0.26484281627031475 Val loss:  0.18320697359740734\n",
      "epoch:  378 Train loss:  0.24912706304054993 Val loss:  0.1824045032262802\n",
      "epoch:  379 Train loss:  0.2518086490722803 Val loss:  0.18585505709052086\n",
      "epoch:  380 Train loss:  0.2531430022074626 Val loss:  0.18289812840521336\n",
      "epoch:  381 Train loss:  0.24393253143017107 Val loss:  0.18344596400856972\n",
      "epoch:  382 Train loss:  0.24114314695963493 Val loss:  0.18205071613192558\n",
      "epoch:  383 Train loss:  0.24476302587069 Val loss:  0.1836287435144186\n",
      "epoch:  384 Train loss:  0.23873696877406195 Val loss:  0.18376949429512024\n",
      "epoch:  385 Train loss:  0.24826367657918197 Val loss:  0.18091602064669132\n",
      "epoch:  386 Train loss:  0.24394931701513436 Val loss:  0.17764338105916977\n",
      "epoch:  387 Train loss:  0.2588812823478992 Val loss:  0.18237828835844994\n",
      "epoch:  388 Train loss:  0.24446554940480453 Val loss:  0.1781354546546936\n",
      "epoch:  389 Train loss:  0.26201726725468266 Val loss:  0.17994097713381052\n",
      "epoch:  390 Train loss:  0.24580596043513372 Val loss:  0.17527526896446943\n",
      "epoch:  391 Train loss:  0.2532174507012734 Val loss:  0.18013743590563536\n",
      "epoch:  392 Train loss:  0.248450728563162 Val loss:  0.17980459053069353\n",
      "epoch:  393 Train loss:  0.24414565643438926 Val loss:  0.18405130598694086\n",
      "epoch:  394 Train loss:  0.2539349909012134 Val loss:  0.17960232682526112\n",
      "epoch:  395 Train loss:  0.24874855692570025 Val loss:  0.1784707624465227\n",
      "epoch:  396 Train loss:  0.2498221878822033 Val loss:  0.17571176309138536\n",
      "epoch:  397 Train loss:  0.24679118394851685 Val loss:  0.1762667503207922\n",
      "epoch:  398 Train loss:  0.2597938443605716 Val loss:  0.17794995196163654\n",
      "epoch:  399 Train loss:  0.240423518877763 Val loss:  0.17857441492378712\n",
      "epoch:  400 Train loss:  0.24209441473850837 Val loss:  0.17528204433619976\n",
      "epoch:  401 Train loss:  0.2505514541497597 Val loss:  0.17664269357919693\n",
      "epoch:  402 Train loss:  0.2390459248652825 Val loss:  0.18199358694255352\n",
      "epoch:  403 Train loss:  0.2410269436927942 Val loss:  0.17540371045470238\n",
      "epoch:  404 Train loss:  0.38935165909620434 Val loss:  0.18233394157141447\n",
      "epoch:  405 Train loss:  0.25025516232618916 Val loss:  0.18705952540040016\n",
      "epoch:  406 Train loss:  0.24872698348302108 Val loss:  0.18124585039913654\n",
      "epoch:  407 Train loss:  0.32788016016666705 Val loss:  0.19466326758265495\n",
      "epoch:  408 Train loss:  0.24440751740565667 Val loss:  0.18731548823416233\n",
      "epoch:  409 Train loss:  0.26858753653673023 Val loss:  0.1893434189260006\n",
      "epoch:  410 Train loss:  0.26908340935523695 Val loss:  0.2088312916457653\n",
      "epoch:  411 Train loss:  0.26792861635868365 Val loss:  0.19524787738919258\n",
      "epoch:  412 Train loss:  0.2501005553282224 Val loss:  0.18358351103961468\n",
      "epoch:  413 Train loss:  0.28270634664939 Val loss:  0.1799131352454424\n",
      "epoch:  414 Train loss:  0.2587300596328882 Val loss:  0.1859783921390772\n",
      "epoch:  415 Train loss:  0.32312149841051835 Val loss:  0.186568733304739\n",
      "epoch:  416 Train loss:  0.25320449012976426 Val loss:  0.19292500242590904\n",
      "epoch:  417 Train loss:  0.2820968490380507 Val loss:  0.18750250153243542\n",
      "epoch:  418 Train loss:  0.2538791344715999 Val loss:  0.18983983621001244\n",
      "epoch:  419 Train loss:  0.255938504750912 Val loss:  0.18908832967281342\n",
      "epoch:  420 Train loss:  0.257072382248365 Val loss:  0.1909872218966484\n",
      "epoch:  421 Train loss:  0.286422284749838 Val loss:  0.19519636780023575\n",
      "epoch:  422 Train loss:  0.2485423214160479 Val loss:  0.2001780830323696\n",
      "epoch:  423 Train loss:  0.26650423270005447 Val loss:  0.19438350945711136\n",
      "epoch:  424 Train loss:  0.2615902859431047 Val loss:  0.2002030797302723\n",
      "epoch:  425 Train loss:  0.2498350624854748 Val loss:  0.19797505252063274\n",
      "epoch:  426 Train loss:  0.25776349351956296 Val loss:  0.19375222362577915\n",
      "epoch:  427 Train loss:  0.23756977457266587 Val loss:  0.1909760907292366\n",
      "epoch:  428 Train loss:  0.2389561373453874 Val loss:  0.18613990768790245\n",
      "epoch:  429 Train loss:  0.2545594068673941 Val loss:  0.18451613374054432\n",
      "epoch:  430 Train loss:  0.26501835882663727 Val loss:  0.18814658373594284\n",
      "epoch:  431 Train loss:  0.24871424642892984 Val loss:  0.19302677735686302\n",
      "epoch:  432 Train loss:  0.256976190667886 Val loss:  0.1969606690108776\n",
      "epoch:  433 Train loss:  0.35899414465977597 Val loss:  0.19546571187675\n",
      "epoch:  434 Train loss:  0.2553285348873872 Val loss:  0.21149595640599728\n",
      "epoch:  435 Train loss:  0.2578350408719136 Val loss:  0.18991430662572384\n",
      "epoch:  436 Train loss:  0.24814909467330346 Val loss:  0.19301142916083336\n",
      "epoch:  437 Train loss:  0.24807452238523042 Val loss:  0.1964205913245678\n",
      "epoch:  438 Train loss:  0.24331792959800133 Val loss:  0.19050606526434422\n",
      "epoch:  439 Train loss:  0.2911228606334099 Val loss:  0.2003764547407627\n",
      "epoch:  440 Train loss:  0.2507199484568376 Val loss:  0.20033152773976326\n",
      "epoch:  441 Train loss:  0.25959380314900327 Val loss:  0.1907750703394413\n",
      "epoch:  442 Train loss:  0.25454619182990146 Val loss:  0.188123669475317\n",
      "epoch:  443 Train loss:  0.27264350538070387 Val loss:  0.19644991122186184\n",
      "epoch:  444 Train loss:  0.24368117635066694 Val loss:  0.19233797118067741\n",
      "epoch:  445 Train loss:  0.2444114272411053 Val loss:  0.18767959997057915\n",
      "epoch:  446 Train loss:  0.23753198408163512 Val loss:  0.18992925435304642\n",
      "epoch:  447 Train loss:  0.26441256128824675 Val loss:  0.18938438408076763\n",
      "epoch:  448 Train loss:  0.25259127754431504 Val loss:  0.18851354345679283\n",
      "epoch:  449 Train loss:  0.24551454874185416 Val loss:  0.188365975394845\n",
      "epoch:  450 Train loss:  0.23837266461207315 Val loss:  0.18821213766932487\n",
      "epoch:  451 Train loss:  0.31319458094926983 Val loss:  0.18325771763920784\n",
      "epoch:  452 Train loss:  0.2501744650877439 Val loss:  0.18416235782206059\n",
      "epoch:  453 Train loss:  0.23337576824885148 Val loss:  0.1783374659717083\n",
      "epoch:  454 Train loss:  0.2505205009992306 Val loss:  0.18353653699159622\n",
      "epoch:  455 Train loss:  0.2520259893857516 Val loss:  0.18257945589721203\n",
      "epoch:  456 Train loss:  0.25162549488819563 Val loss:  0.18296479061245918\n",
      "epoch:  457 Train loss:  0.23590365453408316 Val loss:  0.19373301602900028\n",
      "epoch:  458 Train loss:  0.24905845408256239 Val loss:  0.18299727886915207\n",
      "epoch:  459 Train loss:  0.2439466921182779 Val loss:  0.18031779862940311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  460 Train loss:  0.23859554185317114 Val loss:  0.18739367090165615\n",
      "epoch:  461 Train loss:  0.24486155120226052 Val loss:  0.1897169966250658\n",
      "epoch:  462 Train loss:  0.2682799788621756 Val loss:  0.19656925462186337\n",
      "epoch:  463 Train loss:  0.24302175641059875 Val loss:  0.19285768270492554\n",
      "epoch:  464 Train loss:  0.2512880139625989 Val loss:  0.19267595186829567\n",
      "epoch:  465 Train loss:  0.23430949908036453 Val loss:  0.1821226328611374\n",
      "epoch:  466 Train loss:  0.24096419834173644 Val loss:  0.18521075695753098\n",
      "epoch:  467 Train loss:  0.23399753811267707 Val loss:  0.1867924965918064\n",
      "epoch:  468 Train loss:  0.2374901003562487 Val loss:  0.18474488146603107\n",
      "epoch:  469 Train loss:  0.22974494672738588 Val loss:  0.18415450491011143\n",
      "epoch:  470 Train loss:  0.2421285452751013 Val loss:  0.18401913344860077\n",
      "epoch:  471 Train loss:  0.23868911999922532 Val loss:  0.183345764875412\n",
      "epoch:  472 Train loss:  0.24032818124844477 Val loss:  0.18177585490047932\n",
      "epoch:  473 Train loss:  0.2650614839333754 Val loss:  0.18349949829280376\n",
      "epoch:  474 Train loss:  0.2401546572263424 Val loss:  0.18786821141839027\n",
      "epoch:  475 Train loss:  0.23530979683765998 Val loss:  0.1812825407832861\n",
      "epoch:  476 Train loss:  0.22744208402358568 Val loss:  0.1826843861490488\n",
      "epoch:  477 Train loss:  0.244781494140625 Val loss:  0.17604776099324226\n",
      "epoch:  478 Train loss:  0.24544406281067774 Val loss:  0.17698592133820057\n",
      "epoch:  479 Train loss:  0.2331587984584845 Val loss:  0.1818690188229084\n",
      "epoch:  480 Train loss:  0.2314658331183287 Val loss:  0.18462876416742802\n",
      "epoch:  481 Train loss:  0.2520875953710996 Val loss:  0.17956988885998726\n",
      "epoch:  482 Train loss:  0.24593835266736838 Val loss:  0.18371511716395617\n",
      "epoch:  483 Train loss:  0.22911001971134773 Val loss:  0.184432253241539\n",
      "epoch:  484 Train loss:  0.23485185205936432 Val loss:  0.18121459148824215\n",
      "epoch:  485 Train loss:  0.23449929860922006 Val loss:  0.18407786823809147\n",
      "epoch:  486 Train loss:  0.24872686541997469 Val loss:  0.18402721360325813\n",
      "epoch:  487 Train loss:  0.23601236939430237 Val loss:  0.18432218581438065\n",
      "epoch:  488 Train loss:  0.23944358298411736 Val loss:  0.18580608628690243\n",
      "epoch:  489 Train loss:  0.23100419285205695 Val loss:  0.18385820742696524\n",
      "epoch:  490 Train loss:  0.23509822327357072 Val loss:  0.18539545871317387\n",
      "epoch:  491 Train loss:  0.2361521709423799 Val loss:  0.18258399702608585\n",
      "epoch:  492 Train loss:  0.24208684953359458 Val loss:  0.1860793810337782\n",
      "epoch:  493 Train loss:  0.23264839099003717 Val loss:  0.18724603950977325\n",
      "epoch:  494 Train loss:  0.23699783018002143 Val loss:  0.18346995301544666\n",
      "epoch:  495 Train loss:  0.23782184146917784 Val loss:  0.1826599221676588\n",
      "epoch:  496 Train loss:  0.23248213071089524 Val loss:  0.18545847199857235\n",
      "epoch:  497 Train loss:  0.22895292765819109 Val loss:  0.1840412486344576\n",
      "epoch:  498 Train loss:  0.25088202265592724 Val loss:  0.18318773340433836\n",
      "epoch:  499 Train loss:  0.30105215081801784 Val loss:  0.1902418527752161\n"
     ]
    }
   ],
   "source": [
    "train_loss_hist = []\n",
    "val_loss_hist = []\n",
    "best_loss = 1e10\n",
    "n_epochs = 500\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_data in trn_loader:\n",
    "        \n",
    "        inputs, trues = batch_data\n",
    "        inputs, trues = inputs.cuda(), trues.cuda()\n",
    "        input_c = torch.reshape(trues[:,:,-1], (trues.shape[0], 1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        preds = model(inputs, input_c)\n",
    "        true_ys = torch.reshape(trues[:,:,0:4], (trues.shape[0], 4))\n",
    "        loss = criterion(preds, true_ys).mean(dim=0).mean()\n",
    "        \n",
    "        loss.backward() #偏微分を計算\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "        #パラメータを更新する\n",
    "        optimizer.step()\n",
    "    \n",
    "    val_loss = val_model(epoch)\n",
    "    train_loss_hist.append(train_loss / len(trn_loader))\n",
    "    val_loss_hist.append(val_loss)\n",
    "    \n",
    "    print(\"epoch: \", epoch, \"Train loss: \", train_loss / len(trn_loader), \"Val loss: \", val_loss)\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        save_model_path = exp_dir + \"best_model_\" + str(seq_length) + \"w.pt\"\n",
    "        torch.save(model, save_model_path)\n",
    "        best_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b80a69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(train_loss_hist)), train_loss_hist, c='b', label='train loss')\n",
    "plt.plot(range(len(val_loss_hist)), val_loss_hist, c='r', label='val loss')\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef7f373e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 0.17193219251930714)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(val_loss_hist), np.min(val_loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7b1d63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = exp_dir + \"best_model_\" + str(seq_length) + \"w.pt\"\n",
    "\n",
    "model = torch.load(save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56678d2",
   "metadata": {},
   "source": [
    "### Scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6288d75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n"
     ]
    }
   ],
   "source": [
    "preds_list = []\n",
    "trues_list = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad(): # 勾配計算の無効化\n",
    "    for id, data in enumerate(trn_dataset):\n",
    "        inputs, trues = data\n",
    "        inputs, trues = inputs.cuda(), trues.cuda()\n",
    "        \n",
    "        inputs = torch.reshape(inputs, (1, seq_length, inputs.shape[-1]))\n",
    "        trues = torch.reshape(trues, (1, 1, trues.shape[-1]))\n",
    "        input_c = torch.reshape(trues[:,:,-1], (trues.shape[0], 1))\n",
    "        \n",
    "        preds = model(inputs, input_c)\n",
    "        true_ys = torch.reshape(trues[:,:,0:4], (trues.shape[0], 4))\n",
    "        \n",
    "        print(id)\n",
    "        preds_list.append(preds.to('cpu').detach().numpy().copy())\n",
    "        trues_list.append(true_ys.to('cpu').detach().numpy().copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a6ae9acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((603, 1, 4), (603, 1, 4))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(trues_list).shape, np.array(preds_list).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d438d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba34efa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(np.array(trues_list)[:,:,0], np.array(preds_list)[:,:,0], c = \"b\") # 散布図を描画\n",
    "plt.title(\"Train prediction\")\n",
    "plt.xlabel(\"Trues\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "# y=x の対角線を追加\n",
    "#plt.plot([-2.0, 3.5], [-2.0, 3.5], 'r--')  # 赤の破線で対角線を描画\n",
    "\n",
    "plt.axis(\"equal\")  # X軸とY軸を同じスケールにする\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad4fc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_list = []\n",
    "trues_list = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad(): # 勾配計算の無効化\n",
    "    for id, data in enumerate(val_dataset):\n",
    "        inputs, trues = data\n",
    "        inputs, trues = inputs.cuda(), trues.cuda()\n",
    "        \n",
    "        inputs = torch.reshape(inputs, (1, seq_length, inputs.shape[-1]))\n",
    "        trues = torch.reshape(trues, (1, 1, trues.shape[-1]))\n",
    "        input_c = torch.reshape(trues[:,:,-1], (trues.shape[0], 1))\n",
    "        \n",
    "        preds = model(inputs, input_c)\n",
    "        true_ys = torch.reshape(trues[:,:,0:4], (trues.shape[0], 4))\n",
    "        \n",
    "        print(id)\n",
    "        preds_list.append(preds.to('cpu').detach().numpy().copy())\n",
    "        trues_list.append(true_ys.to('cpu').detach().numpy().copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d8d0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(np.array(trues_list)[:,:,0], np.array(preds_list)[:,:,0], c = \"b\") # 散布図を描画\n",
    "plt.title(\"Val prediction\")\n",
    "plt.xlabel(\"Trues\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "# y=x の対角線を追加\n",
    "#plt.plot([-2.0, 3.5], [-2.0, 3.5], 'r--')  # 赤の破線で対角線を描画\n",
    "\n",
    "plt.axis(\"equal\")  # X軸とY軸を同じスケールにする\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef696ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f600b84e",
   "metadata": {},
   "source": [
    "## Test prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0162aa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_np = np.concatenate([test_ts_np.reshape(-1,1), test_ys_np, test_xs_np.reshape(-1,1)], axis = 1)\n",
    "\n",
    "train_np = np.load(exp_dir + \"train_np.npy\")\n",
    "test_np = np.load(exp_dir + \"test_np.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7565f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_np.shape, test_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70599897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# スケーラーの読み込み\n",
    "std_scaler = joblib.load(data_dir + 'std_scaler.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12b45cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#元のデータの最大値と最小値を指定\n",
    "dat_min = 0\n",
    "dat_max = 3e5\n",
    "\n",
    "#スケール後の最大値と最小値を指定\n",
    "custom_min = 0\n",
    "custom_max = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16ddc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaled_ts = (np.array(test_np[:,0]) - dat_min) / (dat_max - dat_min) * (custom_max - custom_min) + custom_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07be0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#元のデータの最大値と最小値を指定\n",
    "dat_min = 0\n",
    "dat_max = 500\n",
    "\n",
    "#スケール後の最大値と最小値を指定\n",
    "custom_min = 0\n",
    "custom_max = 10\n",
    "\n",
    "test_scaled_zs = (np.array(test_np[:,4]) - dat_min) / (dat_max - dat_min) * (custom_max - custom_min) + custom_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c16aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaled_ys = std_scaler.transform(test_np[:,1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e22839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaled_ys.shape, test_scaled_zs.reshape(-1,1).shape, test_scaled_ts.reshape(-1,1).shape, test_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be05f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all_np = np.concatenate([test_scaled_ts.reshape(-1,1), test_scaled_ys, test_scaled_zs.reshape(-1,1), test_np[:,-1].reshape(-1,1)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ba773",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261520de",
   "metadata": {},
   "source": [
    "### 1. Prediction based on results from 40 days ago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c5e632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def make_sequence_data(data: np.ndarray, seq_size: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \n",
    "    \"\"\"データをsequence_sizeに指定したサイズのシーケンスに分けてシーケンスとその答えをarrayで返す\n",
    "    Args:\n",
    "        data (np.ndarray): 入力データ\n",
    "        seq_size (int): シーケンスサイズ\n",
    "    Returns:\n",
    "        seq_arr: seq_sizeに指定した数のシーケンスを格納するarray\n",
    "        target_arr: シーケンスに対応する答えを格納するarray\n",
    "    \"\"\"\n",
    "\n",
    "    num_data = len(data)\n",
    "    seq_data = []\n",
    "    target_data = []\n",
    "    \n",
    "    for i in range(num_data - seq_size):\n",
    "        seq_data.append(data[i:i+seq_size])\n",
    "        target_data.append(data[i+seq_size:i+seq_size+1])\n",
    "        \n",
    "    seq_arr = np.array(seq_data)\n",
    "    target_arr = np.array(target_data)\n",
    "\n",
    "    return seq_arr, target_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f2f80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Xs, test_ys = make_sequence_data(test_all_np, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12591a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_xs_tensor = torch.Tensor(test_Xs)\n",
    "tst_ys_tensor = torch.Tensor(test_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79a2a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "tst_dataset = TensorDataset(tst_xs_tensor, tst_ys_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b592ca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_list = []\n",
    "trues_list = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad(): # 勾配計算の無効化\n",
    "    for id, data in enumerate(tst_dataset):\n",
    "        inputs, trues = data\n",
    "        inputs, trues = inputs.cuda(), trues.cuda()\n",
    "        \n",
    "        inputs = torch.reshape(inputs, (1, seq_length, inputs.shape[-1]))\n",
    "        trues = torch.reshape(trues, (1, 1, trues.shape[-1]))\n",
    "        input_c = torch.reshape(trues[:,:,-1], (trues.shape[0], 1))\n",
    "        \n",
    "        preds = model(inputs, input_c)\n",
    "        true_ys = torch.reshape(trues[:,:,0:4], (trues.shape[0], 4))\n",
    "        \n",
    "        print(id)\n",
    "        preds_list.append(preds.to('cpu').detach().numpy().copy())\n",
    "        trues_list.append(true_ys.to('cpu').detach().numpy().copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c06368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(np.array(trues_list)[:,:,0], np.array(preds_list)[:,:,0], c = \"b\") # 散布図を描画\n",
    "plt.title(\"Test prediction\")\n",
    "plt.xlabel(\"Trues\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "# y=x の対角線を追加\n",
    "#plt.plot([-2.0, 3.5], [-2.0, 3.5], 'r--')  # 赤の破線で対角線を描画\n",
    "plt.gca().set_aspect('equal', adjustable='datalim')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa2df0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_trues, tst_preds = np.array(trues_list)[:,:,0], np.array(preds_list)[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb54503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#元のデータの最大値と最小値を指定\n",
    "dat_min = 0\n",
    "dat_max = 3e5\n",
    "\n",
    "#スケール後の最大値と最小値を指定\n",
    "custom_min = 0\n",
    "custom_max = 100\n",
    "\n",
    "tst_preds = (np.array(preds_list)[:,:,0] - custom_min) / (custom_max - custom_min) * (dat_max - dat_min) + dat_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2372da03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(test_np[seq_length:,0], tst_preds, c = \"b\") # 散布図を描画\n",
    "plt.title(\"Test prediction (re scaled)\")\n",
    "plt.xlabel(\"Trues\")\n",
    "plt.ylabel(\"Predictions\")\n",
    "# y=x の対角線を追加\n",
    "#plt.plot([-2.0, 3.5], [-2.0, 3.5], 'r--')  # 赤の破線で対角線を描画\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9c8873",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd33b05d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05445ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "plt.style.use('classic')  # スタイリッシュな背景に変更\n",
    "\n",
    "# 実際のデータと予測データをプロット\n",
    "plt.plot(test_steps, df_tokyo[\"Flucases\"][train_n:train_n+test_n], label=\"Actual Test Data\", linestyle='--', linewidth=2, color='blue', alpha=0.8)\n",
    "plt.plot(test_steps[seq_length:], tst_preds, label=\"Prediction\", linestyle='-', linewidth=2, color='red', alpha=1.0)\n",
    "\n",
    "# 軸ラベルとタイトルを追加\n",
    "plt.xlabel(\"Week\", fontsize=20, fontweight='bold')\n",
    "plt.ylabel(\"Flucases\", fontsize=20, fontweight='bold')\n",
    "plt.title(\"Flucases Prediction (aux-vec LSTM)\", fontsize=24, fontweight='bold')\n",
    "\n",
    "# グリッドの調整\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.75, alpha=0.75)\n",
    "\n",
    "# 目盛りのフォントサイズを変更\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "# 凡例の調整\n",
    "plt.legend(fontsize=20, loc='upper left', frameon=True, shadow=True)\n",
    "plt.ylim(bottom=-500)\n",
    "plt.xlim(left=min(test_steps) - 10, right = max(test_steps) + 10)\n",
    "\n",
    "# 仕上げ\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"Flucases Prediction (aux-vec LSTM).jpg\", dpi=200, bbox_inches='tight', format='jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5602a694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    mean_squared_error,  # MSE\n",
    "    mean_absolute_error,  # MAE\n",
    "    r2_score, # R2\n",
    ")\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def reg_metric(trues, preds, name):\n",
    "    mse = mean_squared_error(trues, preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(trues, preds)\n",
    "    r2 = r2_score(trues, preds)\n",
    "    r, p = pearsonr(trues, preds)\n",
    "    \n",
    "    print(name)\n",
    "    print(f'MSE : {mse}.')\n",
    "    print(f'RMSE : {rmse}.')\n",
    "    print(f'MAE : {mae}.')\n",
    "    print(f'R2 : {r2}.')\n",
    "    print(\"pearson :\", r, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3beb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_metric(test_np[seq_length:,0], tst_preds[:,0], \"Flucases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831d760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_metric(test_np[seq_length*2:,0], tst_preds[seq_length:,0], \"Flucases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b5913",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
